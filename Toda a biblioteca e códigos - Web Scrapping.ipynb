{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae78b692",
   "metadata": {},
   "source": [
    "# Descobrir qual tecnologia o site está usando\n",
    "\n",
    "### biblioteca builtwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install builtwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c23120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse significa analisar\n",
    "import builtwith\n",
    "builtwith.parse(\"https://www.facebook.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5be0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "\n",
    "print(whois.whois('https://globo.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui vamos ver quantas vezes a palavra colocada dentro da variável padrão aparece no texto, \n",
    "#no .search poderia ser a palrava eu ou o varíavel como no exemplo\n",
    "\n",
    "#^é para encontrar padrões em começo de texto, se eu colocasse êu ele iria considerar só o primeiro eu porque está\n",
    "#no começo, o $ é o fim do texto. Se eu quiser encontrar um caracter específico com o ponto \".\" eu uso a contra\n",
    "#barra \\ \n",
    "\n",
    "#re.search(padrão, texto)eu colocar .findall ele vai me dar todas as ocorrências do que estou procurando\n",
    "\n",
    "import re\n",
    "\n",
    "texto = 'eu digo pra eu que eu sou foda porque eu sei muita coisa'\n",
    "\n",
    "padrão='eu'\n",
    "\n",
    "re.search(padrão, texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ba4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pesquisar CNPJ\n",
    "\n",
    "import re\n",
    "\n",
    "padrao = re.compile(\"([0-9]{2}[\\.]?[0-9]{3}[\\.]?[0-9]{3}[/]?[0-9]{4}[-]?[0-9]{2})|([0-9]{3}[\\.]?[0-9]{3}[\\.]?[0-9]{3}[-]?[0-9]{2})\")\n",
    "#padrao = re.compile(\"([0-9]{2}\\.?[0-9]{3}\\.?[0-9]{3}[/]?[0-9]{4}[-]?[0-9]{2})|([0-9]{3}\\.?[0-9]{3}\\.?[0-9]{3}[-]?[0-9]{2})\")\n",
    "#padrao = re.compile(\"([0-9]{2}.?[0-9]{3}.?[0-9]{3}[/]?[0-9]{4}[-]?[0-9]{2})|([0-9]{3}.?[0-9]{3}.?[0-9]{3}[-]?[0-9]{2})\")\n",
    "\n",
    "# CNPJ: ([0-9]{2}[\\.]?[0-9]{3}[\\.]?[0-9]{3}[\\/]?[0-9]{4}[-]?[0-9]{2})\n",
    "# CPF: ([0-9]{3}[\\.]?[0-9]{3}[\\.]?[0-9]{3}[-]?[0-9]{2})\n",
    "\n",
    "# Texto retirado do documento concurso_novo129066_a87211e5a81747267b10c00f8d836a44.pdf\n",
    "# com o resultado de um concurso que pode ser localizado na internet de forma pública\n",
    "texto = (r\"\"\"Nº INSC. CANDIDATO CARGO C P F\n",
    "RELAÇÃO DE CANDIDATOS - CONCURSO PÚBLICO\n",
    "42 FABIANO FINGER SANTOS ODONTOLÓGO 00045847916 !!! SEM PONTUAÇÃO\n",
    "35 FERNANDA REGINA LOTTI ODONTOLÓGO 12.345.444/7777-85 !!!!FORMATO CNPJ\n",
    "70 FRANCIANE GOMES ODONTOLÓGO 049.105.969-84\n",
    "87 GABRIELA REBÊLO ODONTOLÓGO 066.357.219-32\n",
    "82 GUILHERME AUGUSTO TREVISOL ODONTOLÓGO 070.846.319-33\n",
    "36 JACKELINE DELITSCH ODONTOLÓGO 046.692.069-58\n",
    "29 JOCIE GERALDO FRATTINI ODONTOLÓGO 006.464.199-60\n",
    "39 JONATHAN WOLINGER DA ROCHA LOURES ODONTOLÓGO 040.613.289-56\n",
    "91 JULIANA OLIVEIRA FORNARI ODONTOLÓGO 032.359.259-70\n",
    "30 KEYLLA WITTMANN ODONTOLÓGO 044.480.669-59\n",
    "5 LAURA CRISTINA CAZZAMALLI ODONTOLÓGO 041.365.099-52\n",
    "32 LUANA PARISOTTO ODONTOLÓGO 047.646.129-41\n",
    "51 LUCIANO FERNANDES VALOTA ODONTOLÓGO 948.324.806-00\n",
    "9 LUCIANO MENEGAT COLOMBELLI ODONTOLÓGO 981.097.580-53\n",
    "68 LUIZ EDUARDO CORREA RODRIGUEZ ODONTOLÓGO 530.428.919-68\n",
    "67 LUIZ FELIPE SANTOS PEREIRA ODONTOLÓGO 06318824995 !!!!!CPF SEM MÁSCARA\n",
    "44 LUIZ OMAR WEILLER ODONTOLÓGO 828.400.590-53\n",
    "31 MARCOS VINICIUS PARISOTTO ODONTOLÓGO 064.390.099-31\n",
    "71 MARCUS PALMA NUNES ODONTOLÓGO 042.393.449-05\n",
    "75 MARIANA MATOS KOWALSKI ODONTOLÓGO 066.165.739-66\n",
    "80 MARIANE VENANCIO ANDRADE PINTO ODONTOLÓGO 058.503.489-30\n",
    "38 MARLON ANDRE MAZARO ODONTOLÓGO 054.416.209-94\n",
    "66 MATEUS CAMPOS VENTURA ODONTOLÓGO 053.675.399-71\n",
    "33 MAURICIO SANDINI FURLAN ODONTOLÓGO 050.992.079-95\n",
    "63 MONICA RIZZI ODONTOLÓGO 007.927.379-38\n",
    "18 NATALIA DORINI ODONTOLÓGO 047.914.709-40\n",
    "8 PAULO RICARDO CORRÊA ODONTOLÓGO 923.662.369-72\n",
    "XX FULANO DE TAL 12.345.678/0001-85 !!!!!UM CNPJ\n",
    "\"\"\")\n",
    "\n",
    "resultado = re.findall(padrao, texto)\n",
    "print(\"CPF/CNPJ localizados no texto:\", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encontrar e-mail\n",
    "\n",
    "import re\n",
    "\n",
    "padrao = r'[\\w.-]+@[\\w.-]+'\n",
    "texto = r\"Meu principal e-mail é evaldowolkers@gmail.com, mas tenho também o evaldorw@hotmail.com, e o que dizer do evaldo.wolkers@algumacoisa.com.br? Este eu ainda não tenho. Que tal também o evaldo-wolkers@algo.com.br?\"\n",
    "match = re.findall(padrao, texto)\n",
    "print(match)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\\w -> Caractere alfanumérico ou sublinhado [a-zA-Z0-9_]\n",
    ". -> Considera o \".\". Devido ao ponto, foi encontrado \"evaldo.wolkers@algumacoisa.com.br\", se não permitisse o ponto, seria retornado 'wolkers@algumacoisa.com.br'.\n",
    "- -> Considera o \"-\". Devido ao hífen, foi encontrado \"evaldo-wolkers@algo.com.br\", se não permitisse o ponto, seria retornado 'wolkers@algo.com.br'.\n",
    "+ -> Uma ou mais ocorrências da expressão anterior [\\w.-]\n",
    "@ -> Considera um caractere arroba.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://beatco.com.br/\")\n",
    "s=html.read()\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9960fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b10d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#esse comando deve ser rodado no terminal, buscar uma pastar para instalar o servidor usando cd e o nome da pasta\n",
    "#usar cd .. (com espaço mesmo) para voltar ao root. Depois de rodar o comando ir até o navegador e digitar\n",
    "#localhost:8000\n",
    "\n",
    "python -m http.server 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4615396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://www.beatco.com.br\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "\n",
    "print(pagina.h1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb76d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se eu usar find_all vou ter o resultado de todas as instâncias que eu procurar \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://beatco.com.br\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "\n",
    "print(pagina.find_all(\"h1\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar links, tag a busca todos os links de uma página\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://localhost:8000/My%20Drive/Python%20Web%20Scrapping/teste2.html\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "\n",
    "print(pagina.find_all(\"a\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e341b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar links, tag a busca todos os links de uma página\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://localhost:8000/My%20Drive/Python%20Web%20Scrapping/teste2.html\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "links=pagina.find_all(\"a\")\n",
    "\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a438950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar links, usando o link.get eu pego só os links\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://localhost:8000/My%20Drive/Python%20Web%20Scrapping/teste2.html\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "links=pagina.find_all(\"a\")\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e132734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar links, usando o link.get eu pego só os links\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://beatco.com.br\")\n",
    "s=html.read()\n",
    "pagina=BeautifulSoup(s,\"html.parser\")\n",
    "links=pagina.find_all(\"a\")\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui temos o código completo, ele tenta buscar as htmls e no primeiro ele procura um erro de HTTP, ou seja, página\n",
    "#não existe, mas o servidor existe, já no segundo exemplo, o domínio não existe, no terceiro erro ele tenta imprimir\n",
    "#uma tag que não existe, tag é por exemplo o head, se não existir ele traz um erro none.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#busca título\n",
    "def getTitulo(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as erro:\n",
    "        print(f\"Ocorreu um erro HTTP: {erro}\")\n",
    "        return None\n",
    "    except URLError as erro:\n",
    "        print(f\"Ocorreu um erro de URL: {erro}\")\n",
    "        return None\n",
    "    except:\n",
    "        print(f\"Ocorreu um erro ao acessar a página.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"html.parser\")\n",
    "        titulo = bsObj.body.h1\n",
    "    except AttributeError as erro:\n",
    "        print(f\"Ocorreu um erro ao acessar o atributo.\")\n",
    "        return None\n",
    "    except:\n",
    "        print(f\"Ocorreu um erro ao acessar os atributos.\")\n",
    "        return None\n",
    "\n",
    "    return titulo\n",
    "\n",
    "titulo = getTitulo(input(\"Informe a URL: \"))\n",
    "\n",
    "if titulo is None:\n",
    "    print(\"Título não encontrado.\")\n",
    "else:\n",
    "    print(titulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicas de comando\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://localhost:8000/site.html\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# O método prettify \"embelezar\" organiza\n",
    "# o conteúdo do parser mostrando\n",
    "# uma árvore com as tags HTML/XML\n",
    "print(\"Veja o conteúdo com o prettify\")\n",
    "print(soup.prettify())\n",
    "\n",
    "print(\"\\nVeja o conteúdo sem o prettify\")\n",
    "print(soup)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Título do documento:\")\n",
    "print(soup.title)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Nome da tag título:\")\n",
    "print(soup.title.name)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Texto da tag título:\")\n",
    "print(soup.title.string)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Tag pai da tag title:\")\n",
    "print(soup.title.parent)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Nome da tag pai da tag title:\")\n",
    "print(soup.title.parent.name)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Primeira tag 'p' do Texto:\")\n",
    "print(soup.p)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Conteúdo da propriedade class da primeira tag p:\")\n",
    "print(soup.p['class'])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Primeira ocorrência da tag 'a':\")\n",
    "print(soup.a)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Todas ocorrências da tag 'a':\")\n",
    "print(soup.findAll('a'))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Buscando uma tag denominada cujo id seja link3:\")\n",
    "print(soup.find(id=\"link3\"))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Pegando todos os links do documento:\")\n",
    "for link in soup.findAll('a'):\n",
    "    print(link.get('href'))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Pegando todos os textos do documento:\")\n",
    "print(soup.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ecb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://evaldowolkers.wordpress.com\")\n",
    "\n",
    "bsObj = BeautifulSoup(html.read(),\"html.parser\")\n",
    "\n",
    "# .get_text() - Retorna todo o texto da página\n",
    "print(bsObj.get_text())\n",
    "\n",
    "# .tag - Retorna a primeira ocorrência da tag informada\n",
    "print(bsObj.title)\n",
    "\n",
    "# .tag.name - Retorna o nome da primeira ocorrência da tag informada\n",
    "print(bsObj.title.name)\n",
    "\n",
    "# Alterando o nome da tag título\n",
    "bsObj.title.name = \"titulo\"\n",
    "\n",
    "#Visualizando o nome da tag titulo (que era title)\n",
    "print(bsObj.titulo.name)\n",
    "\n",
    "# .tag['atributo'] - retorna todos os valores do atributo informado\n",
    "print(bsObj.body['class'])\n",
    "\n",
    "# .find(id=\"descricao\") - retorna a tag que possua o id informado\n",
    "print(bsObj.find(id=\"menu-item-147\"))\n",
    "\n",
    "# .attrs retorna um dicionário com os atributos\n",
    "print(bsObj.meta.attrs)\n",
    "\n",
    "print(\"*********\")\n",
    "print(bsObj.name)\n",
    "\n",
    "html = urlopen(\"https://evaldowolkers.wordpress.com\")\n",
    "bsobj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Localizar qualquer tag que tenha a propriedade class igual a comments-link\n",
    "teste = bsobj.findAll(\"\", {\"class\":\"comments-link\"})\n",
    "for a in teste:\n",
    "    print(a)\n",
    "\n",
    "python = bsobj.findAll(text=\"Python\")\n",
    "for a in python:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui eu procuro todos os links que contenham no link essas palavras\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "paginas = set()\n",
    "paginas_invalidas = set()\n",
    "nova_pagina = \"\"\n",
    "\n",
    "def abrir_pagina(url_da_pagina):\n",
    "    global paginas\n",
    "    try:\n",
    "        if url_da_pagina not in paginas_invalidas:\n",
    "            html = urlopen(url_da_pagina)\n",
    "            bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "            serie_a_g6_2017 = ('.corinthians.|.gremio.|.santos.|.palmeiras.|.flamengo.|.cruzeiro.')\n",
    "\n",
    "            for link in bsObj.findAll(\"a\", href=re.compile(serie_a_g6_2017)):\n",
    "                if \"href\" in link.attrs:\n",
    "                    if link.attrs['href'] not in paginas and link.attrs['href'] not in paginas_invalidas:\n",
    "                        nova_pagina = link.attrs['href']\n",
    "                        print(nova_pagina)\n",
    "                        paginas.add(nova_pagina)\n",
    "                        abrir_pagina(nova_pagina)\n",
    "    except:\n",
    "        paginas_invalidas.add(nova_pagina)\n",
    "\n",
    "abrir_pagina(\"http://globoesporte.globo.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui eu procuro todas as ocorrências img que contenham o padrão .. indicados pelo \\barra ao contrário que indica\n",
    "#que o é literalmente aquele ponto seguido do {2} que indica que são dois caracteres do que está antes dele, nesse\n",
    "#caso o ponto duas vezes. Depois a sequência que eu vi como padrão até o img e após ele eu indico pelo d que vai ter\n",
    "#um número de 0 a 9 e coloco o * pra dizer que podem ocorrer zero ou muitas ocorrências do d ou seja muitos 0 ou 9\n",
    "#logo após coloca o jpg como as imagens estão salvas\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "imagens = soup.findAll(\"img\", {\"src\":re.compile(\"\\.{2}/img/gifts/img\\d*\\.jpg\")})\n",
    "\n",
    "for img in imagens:\n",
    "     print(img[\"src\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef37dbba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Request.__init__() got an unexpected keyword argument 'header'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Erro 403 porque alguns sites tratam scraping\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#html = urlopen(\"http://www.tudogostoso.com.br\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# sistema operacional, fornecedor do software ou versão do software do agente\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# de usuário do software solicitante.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m req \u001b[39m=\u001b[39m Request(\u001b[39m\"\u001b[39;49m\u001b[39mhttp://www.tudogostoso.com.br\u001b[39;49m\u001b[39m\"\u001b[39;49m, header \u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mUser-Agent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mX-Requested-With\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mXMLHttpRequest\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m     17\u001b[0m html \u001b[39m=\u001b[39m urlopen(req)\u001b[39m.\u001b[39mread()\n\u001b[1;32m     19\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Request.__init__() got an unexpected keyword argument 'header'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Erro 403 porque alguns sites tratam scraping\n",
    "#html = urlopen(\"http://www.tudogostoso.com.br\")\n",
    "\n",
    "# Conteúdo sobre user-agent:\n",
    "# https://developer.mozilla.org/pt-BR/docs/Web/HTTP/Headers/User-Agent\n",
    "# O cabeçalho de requisição User-Agent contém uma string característica\n",
    "# que permite o protocolo de rede do cliente identificar o tipo de aplicação,\n",
    "# sistema operacional, fornecedor do software ou versão do software do agente\n",
    "# de usuário do software solicitante.\n",
    "\n",
    "req = Request(\"http://www.tudogostoso.com.br\", header = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\"X-Requested-With\": \"XMLHttpRequest\"})\n",
    "\n",
    "html = urlopen(req).read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "links = soup.findAll(\"a\", {\"href\":re.compile(\"/categorias/.*\\.php\")})\n",
    "\n",
    "for link in links:\n",
    "    print(link[\"href\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Quero só os links do folha.uol do mês 11 de 2017, da categoria mercado\n",
    "html = urlopen(\"https://www.folha.uol.com.br/\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "links = soup.findAll(\"a\", {\"href\":re.compile(\".*\\.folha\\.uol\\.com\\.br/mercado/2022/01/.*\\.shtml\")})\n",
    "\n",
    "for link in links:\n",
    "    print(link[\"href\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51731c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de clientes: 3\n",
      "Tag clientes[1]: cliente2\n",
      "cliente1\n",
      "cliente2\n",
      "cliente3\n",
      "<clientes>\n",
      "  <cliente1>\n",
      "    <nome>Fulano de Marte</nome>\n",
      "  </cliente1>\n",
      "  <cliente2>\n",
      "    <nome>Anabelle de Saturno</nome>\n",
      "  </cliente2>\n",
      "  <cliente3>\n",
      "    <nome>Plutonilda</nome>\n",
      "  </cliente3>\n",
      "</clientes>\n",
      "\n",
      "<clientes>\n",
      "  <cliente0/>\n",
      "  <cliente1>\n",
      "    <nome>Fulano de Marte</nome>\n",
      "  </cliente1>\n",
      "  <cliente2>\n",
      "    <nome>Anabelle de Saturno</nome>\n",
      "  </cliente2>\n",
      "  <cliente3>\n",
      "    <nome>Plutonilda</nome>\n",
      "  </cliente3>\n",
      "</clientes>\n",
      "\n",
      "cliente1\n",
      "cliente2\n",
      "<clientes>\n",
      "  <cliente0/>\n",
      "  <cliente1>\n",
      "    <nome>Fulano de Marte</nome>\n",
      "  </cliente1>\n",
      "  <cliente2>\n",
      "    <nome>Anabelle de Saturno</nome>\n",
      "  </cliente2>\n",
      "  <cliente3>\n",
      "    <nome>Plutonilda</nome>\n",
      "  </cliente3>\n",
      "</clientes>\n",
      "\n",
      "<clientes>\n",
      "  <cliente0/>\n",
      "  <cliente1>\n",
      "    <nome>Fulano de Marte</nome>\n",
      "  </cliente1>\n",
      "  <cliente3>\n",
      "    <nome>Plutonilda</nome>\n",
      "  </cliente3>\n",
      "</clientes>\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#lendo xml\n",
    "\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "clientes = etree.Element(\"clientes\")\n",
    "\n",
    "cliente1 = etree.SubElement(clientes, \"cliente1\")\n",
    "nome1 = etree.SubElement(cliente1, \"nome\")\n",
    "nome1.text = \"Fulano de Marte\"\n",
    "\n",
    "cliente2 = etree.Element(\"cliente2\")\n",
    "nome2 = etree.SubElement(cliente2, \"nome\")\n",
    "nome2.text = \"Anabelle de Saturno\"\n",
    "clientes.append(cliente2)\n",
    "\n",
    "cliente3 = etree.Element(\"cliente3\")\n",
    "nome3 = etree.SubElement(cliente3, \"nome\")\n",
    "nome3.text = \"Plutonilda\"\n",
    "clientes.append(cliente3)\n",
    "\n",
    "# len(Element) retorna a quantidade de tags do objeto\n",
    "print(\"Total de clientes:\", len(clientes))\n",
    "\n",
    "# Criando um objeto com base no segundo elemento\n",
    "cliente_dois = clientes[1]\n",
    "# Imprimindo a tag do elemento de índice 1, que é\n",
    "print(\"Tag clientes[1]:\", cliente_dois.tag)\n",
    "\n",
    "# Percorrento o Element como uma lista\n",
    "for x in clientes:\n",
    "    print(x.tag)\n",
    "\n",
    "print(etree.tostring(clientes, pretty_print=True).decode(\"utf-8\"))\n",
    "\n",
    "# Inserindo um elemento usando insert\n",
    "clientes.insert(0, etree.Element(\"cliente0\"))\n",
    "print(etree.tostring(clientes, pretty_print=True).decode(\"utf-8\"))\n",
    "\n",
    "# Fatiando a lista clientes, pegando os elementos de 1 a 2\n",
    "# porque [1:3] não inclui o 3\n",
    "fatia1 = clientes[1:3]\n",
    "for x in fatia1:\n",
    "    print(x.tag)\n",
    "\n",
    "print(etree.tostring(clientes, pretty_print=True).decode(\"utf-8\"))\n",
    "# O elemento da posição 2 (cliente2) vai ser substituído\n",
    "# pelo elemento da posição 1 cliente1, ficando apenas os\n",
    "# elementos 0, 1 e 3\n",
    "clientes[2] = clientes[1]\n",
    "print(etree.tostring(clientes, pretty_print=True).decode(\"utf-8\"))\n",
    "\n",
    "# Verifica se clientes é 'pai' de clientes[1]\n",
    "print(clientes is clientes[1].getparent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_lista_de_deputados\n",
    "    dados=etree.parse(\"Ano2017.xml\")\n",
    "    lista_despesas=dados.findall(\"DESPESAS\")\n",
    "    \n",
    "    lista_deputados=[]\n",
    "    \n",
    "    for despesa in lista_despesa:\n",
    "        for informacao in despesa:\n",
    "            registros=informacao.getchildren()\n",
    "            deputado=registros[0].text\n",
    "            if deputado in not lista_deputados:\n",
    "                lista_deputados.append(deputado)\n",
    "                \n",
    "    return lista_deputados\n",
    "\n",
    "def carregar_despesas(deputado):\n",
    "    categorias={}\n",
    "    dados=etree.parse(\"Ano2017.xml\")\n",
    "    todas_despesas=dados.findall(\"DESPESAS\")\n",
    "    for despesas in todas_despesas:\n",
    "        desp=despesa.getchildren()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
